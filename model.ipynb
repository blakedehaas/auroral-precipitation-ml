{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxDEc4YBvlN-"
      },
      "source": [
        "Aurora Precipitation ML Notebook\n",
        "\n",
        "Problem: Unstable convergence\n",
        "\n",
        "Ideas:\n",
        "- [Works] Overfit to a single sample first\n",
        "- Increase network size\n",
        "  - Tried a 5 layer FF network, no improvement.\n",
        "- Huber loss (https://pytorch.org/docs/stable/generated/torch.nn.HuberLoss.html)\n",
        "  - I don't think this will improve model performance\n",
        "- [Works] normalize (mean = 0, std dev = 1) all input features\n",
        "- [Works] Scale output features (log scale?)\n",
        "  - Applied log then followed by mean = 0, std dev = 1 on the log values\n",
        "- Make the model output a gaussian distribution as opposed to a single node. Loss is GaussianNLL (https://pytorch.org/docs/stable/generated/torch.nn.GaussianNLLLoss.html)\n",
        "  - This seems to work decently but I'm getting unstable training, have to debug this.\n",
        "- Output multiple gaussian distributions (mixture density network)\n",
        "- k-fold cross-validation\n",
        "(By using multiple training and testing cycles, it minimizes the risk of overfitting to a particular data split)\n",
        "- Look into ensemble of models with different initial conditions / handling of outliers\n",
        "(model ensembles are used a lot in ATOC research since systems are so chaotic)\n",
        "\n",
        "Things to do:\n",
        "- Split out the test set into its own TSV so it stays constant every run.\n",
        "(This wouldn't work with k-fold cross-validation)\n",
        "- Add one cycle LR scheduling\n",
        "- Filter the dataset according to Blake's suggestions\n",
        "- Increase dataloder workers to num cpus\n",
        "- Debug GaussianNetwork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4mr26x9XlJPg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KqscDMaClU3p"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('data/train.tsv', sep='\\t')\n",
        "eval_df = pd.read_csv('data/validation.tsv', sep='\\t')\n",
        "train_df = train_df.sample(frac=0.1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Altitude</th>\n",
              "      <th>GCLAT</th>\n",
              "      <th>GCLON</th>\n",
              "      <th>ILAT</th>\n",
              "      <th>GLAT</th>\n",
              "      <th>GMLT</th>\n",
              "      <th>XXLAT</th>\n",
              "      <th>XXLON</th>\n",
              "      <th>Te1</th>\n",
              "      <th>Ne1</th>\n",
              "      <th>Pv1</th>\n",
              "      <th>I1</th>\n",
              "      <th>DateFormatted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3590.0</td>\n",
              "      <td>33.85</td>\n",
              "      <td>250.0</td>\n",
              "      <td>41.90</td>\n",
              "      <td>22.30</td>\n",
              "      <td>4.12</td>\n",
              "      <td>42.20</td>\n",
              "      <td>4.1</td>\n",
              "      <td>2405</td>\n",
              "      <td>2671</td>\n",
              "      <td>2.34</td>\n",
              "      <td>82</td>\n",
              "      <td>1992-07-30 12:13:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7971.0</td>\n",
              "      <td>-42.04</td>\n",
              "      <td>122.7</td>\n",
              "      <td>55.21</td>\n",
              "      <td>-31.97</td>\n",
              "      <td>5.75</td>\n",
              "      <td>-55.74</td>\n",
              "      <td>5.7</td>\n",
              "      <td>3794</td>\n",
              "      <td>952</td>\n",
              "      <td>2.81</td>\n",
              "      <td>48</td>\n",
              "      <td>1993-07-08 21:16:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7810.0</td>\n",
              "      <td>54.62</td>\n",
              "      <td>101.6</td>\n",
              "      <td>48.74</td>\n",
              "      <td>13.35</td>\n",
              "      <td>5.54</td>\n",
              "      <td>49.67</td>\n",
              "      <td>5.5</td>\n",
              "      <td>3521</td>\n",
              "      <td>1601</td>\n",
              "      <td>2.34</td>\n",
              "      <td>51</td>\n",
              "      <td>1997-05-05 22:24:23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Altitude  GCLAT  GCLON   ILAT   GLAT  GMLT  XXLAT  XXLON   Te1   Ne1   Pv1  \\\n",
              "0    3590.0  33.85  250.0  41.90  22.30  4.12  42.20    4.1  2405  2671  2.34   \n",
              "1    7971.0 -42.04  122.7  55.21 -31.97  5.75 -55.74    5.7  3794   952  2.81   \n",
              "2    7810.0  54.62  101.6  48.74  13.35  5.54  49.67    5.5  3521  1601  2.34   \n",
              "\n",
              "   I1       DateFormatted  \n",
              "0  82 1992-07-30 12:13:09  \n",
              "1  48 1993-07-08 21:16:58  \n",
              "2  51 1997-05-05 22:24:23  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Keep only specific columns\n",
        "columns_to_keep = ['Altitude', 'GCLAT', 'GCLON', 'ILAT', 'GLAT', 'GMLT', 'XXLAT', 'XXLON', 'Te1', 'Ne1', 'Pv1', 'I1', 'DateFormatted', 'TimeFormatted']\n",
        "train_df = train_df[columns_to_keep]\n",
        "eval_df = eval_df[columns_to_keep]\n",
        "\n",
        "# Convert DateFormatted and TimeFormatted columns to datetime\n",
        "train_df['DateFormatted'] = pd.to_datetime(train_df['DateFormatted'] + ' ' + train_df['TimeFormatted'], format='%Y-%m-%d %H:%M:%S')\n",
        "eval_df['DateFormatted'] = pd.to_datetime(eval_df['DateFormatted'] + ' ' + eval_df['TimeFormatted'], format='%Y-%m-%d %H:%M:%S')\n",
        "train_df.drop('TimeFormatted', axis=1, inplace=True)\n",
        "eval_df.drop('TimeFormatted', axis=1, inplace=True)\n",
        "\n",
        "train_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "CilXn9liwKBJ",
        "outputId": "a7f959fc-7db2-46ba-b649-3d4dbfbf5cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalized data shape: (420524, 13)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Altitude</th>\n",
              "      <th>GCLAT</th>\n",
              "      <th>GCLON</th>\n",
              "      <th>ILAT</th>\n",
              "      <th>GLAT</th>\n",
              "      <th>GMLT</th>\n",
              "      <th>XXLAT</th>\n",
              "      <th>XXLON</th>\n",
              "      <th>Te1</th>\n",
              "      <th>Ne1</th>\n",
              "      <th>Pv1</th>\n",
              "      <th>I1</th>\n",
              "      <th>DateFormatted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.661607</td>\n",
              "      <td>0.297887</td>\n",
              "      <td>0.851725</td>\n",
              "      <td>-0.240857</td>\n",
              "      <td>0.335100</td>\n",
              "      <td>-1.029631</td>\n",
              "      <td>-0.037926</td>\n",
              "      <td>-0.220474</td>\n",
              "      <td>-1.277877</td>\n",
              "      <td>0.049650</td>\n",
              "      <td>-0.114876</td>\n",
              "      <td>2.215002</td>\n",
              "      <td>1992-07-30 12:13:09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.053990</td>\n",
              "      <td>-1.252598</td>\n",
              "      <td>-0.347448</td>\n",
              "      <td>-0.030438</td>\n",
              "      <td>-1.163535</td>\n",
              "      <td>-0.802881</td>\n",
              "      <td>-0.596554</td>\n",
              "      <td>-0.211031</td>\n",
              "      <td>-0.791926</td>\n",
              "      <td>-0.267230</td>\n",
              "      <td>0.656657</td>\n",
              "      <td>0.375877</td>\n",
              "      <td>1993-07-08 21:16:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.990942</td>\n",
              "      <td>0.722233</td>\n",
              "      <td>-0.546212</td>\n",
              "      <td>-0.132723</td>\n",
              "      <td>0.087951</td>\n",
              "      <td>-0.832094</td>\n",
              "      <td>0.004681</td>\n",
              "      <td>-0.212212</td>\n",
              "      <td>-0.887437</td>\n",
              "      <td>-0.147594</td>\n",
              "      <td>-0.114876</td>\n",
              "      <td>0.538152</td>\n",
              "      <td>1997-05-05 22:24:23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Altitude     GCLAT     GCLON      ILAT      GLAT      GMLT     XXLAT  \\\n",
              "0 -0.661607  0.297887  0.851725 -0.240857  0.335100 -1.029631 -0.037926   \n",
              "1  1.053990 -1.252598 -0.347448 -0.030438 -1.163535 -0.802881 -0.596554   \n",
              "2  0.990942  0.722233 -0.546212 -0.132723  0.087951 -0.832094  0.004681   \n",
              "\n",
              "      XXLON       Te1       Ne1       Pv1        I1       DateFormatted  \n",
              "0 -0.220474 -1.277877  0.049650 -0.114876  2.215002 1992-07-30 12:13:09  \n",
              "1 -0.211031 -0.791926 -0.267230  0.656657  0.375877 1993-07-08 21:16:58  \n",
              "2 -0.212212 -0.887437 -0.147594 -0.114876  0.538152 1997-05-05 22:24:23  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Normalize all location and atmospheric parameters (mean = 0 and std dev = 1)\n",
        "columns_to_normalize = ['Altitude', 'GCLAT', 'GCLON', 'ILAT', 'GLAT', 'GMLT', 'XXLAT', 'XXLON', 'Ne1', 'Pv1', 'I1', 'Te1']\n",
        "\n",
        "# Function to calculate mean and std dev for specified columns\n",
        "def calculate_stats(df, columns):\n",
        "    means = df[columns].mean()\n",
        "    stds = df[columns].std()\n",
        "    return means, stds\n",
        "\n",
        "# Function to normalize specified columns in the DataFrame\n",
        "def normalize_df(df, means, stds, columns):\n",
        "    df[columns] = (df[columns] - means) / stds\n",
        "    return df\n",
        "\n",
        "# Calculate mean and std for the specified columns\n",
        "means, stds = calculate_stats(train_df, columns_to_normalize)\n",
        "train_df_norm = normalize_df(train_df, means, stds, columns_to_normalize)\n",
        "\n",
        "means, stds = calculate_stats(eval_df, columns_to_normalize)\n",
        "eval_df_norm = normalize_df(eval_df, means, stds, columns_to_normalize)\n",
        "\n",
        "# Verify there are no NaNs in the normalized DataFrame\n",
        "assert train_df_norm[columns_to_normalize].isna().sum().sum() == 0, \"NaN values found in normalized data\"\n",
        "assert eval_df_norm[columns_to_normalize].isna().sum().sum() == 0, \"NaN values found in normalized data\"\n",
        "\n",
        "print(\"Normalized data shape:\", train_df_norm.shape)\n",
        "train_df_norm.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cyclic features added to train and eval datasets.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Altitude</th>\n",
              "      <th>GCLAT</th>\n",
              "      <th>GCLON</th>\n",
              "      <th>ILAT</th>\n",
              "      <th>GLAT</th>\n",
              "      <th>GMLT</th>\n",
              "      <th>XXLAT</th>\n",
              "      <th>XXLON</th>\n",
              "      <th>Te1</th>\n",
              "      <th>Ne1</th>\n",
              "      <th>Pv1</th>\n",
              "      <th>I1</th>\n",
              "      <th>Year</th>\n",
              "      <th>DayOfYear_sin</th>\n",
              "      <th>TimeOfDay_sin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.661607</td>\n",
              "      <td>0.297887</td>\n",
              "      <td>0.851725</td>\n",
              "      <td>-0.240857</td>\n",
              "      <td>0.335100</td>\n",
              "      <td>-1.029631</td>\n",
              "      <td>-0.037926</td>\n",
              "      <td>-0.220474</td>\n",
              "      <td>-1.277877</td>\n",
              "      <td>0.049650</td>\n",
              "      <td>-0.114876</td>\n",
              "      <td>2.215002</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.484089</td>\n",
              "      <td>-0.057346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.053990</td>\n",
              "      <td>-1.252598</td>\n",
              "      <td>-0.347448</td>\n",
              "      <td>-0.030438</td>\n",
              "      <td>-1.163535</td>\n",
              "      <td>-0.802881</td>\n",
              "      <td>-0.596554</td>\n",
              "      <td>-0.211031</td>\n",
              "      <td>-0.791926</td>\n",
              "      <td>-0.267230</td>\n",
              "      <td>0.656657</td>\n",
              "      <td>0.375877</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.109446</td>\n",
              "      <td>-0.652870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.990942</td>\n",
              "      <td>0.722233</td>\n",
              "      <td>-0.546212</td>\n",
              "      <td>-0.132723</td>\n",
              "      <td>0.087951</td>\n",
              "      <td>-0.832094</td>\n",
              "      <td>0.004681</td>\n",
              "      <td>-0.212212</td>\n",
              "      <td>-0.887437</td>\n",
              "      <td>-0.147594</td>\n",
              "      <td>-0.114876</td>\n",
              "      <td>0.538152</td>\n",
              "      <td>8</td>\n",
              "      <td>0.836733</td>\n",
              "      <td>-0.405208</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Altitude     GCLAT     GCLON      ILAT      GLAT      GMLT     XXLAT  \\\n",
              "0 -0.661607  0.297887  0.851725 -0.240857  0.335100 -1.029631 -0.037926   \n",
              "1  1.053990 -1.252598 -0.347448 -0.030438 -1.163535 -0.802881 -0.596554   \n",
              "2  0.990942  0.722233 -0.546212 -0.132723  0.087951 -0.832094  0.004681   \n",
              "\n",
              "      XXLON       Te1       Ne1       Pv1        I1  Year  DayOfYear_sin  \\\n",
              "0 -0.220474 -1.277877  0.049650 -0.114876  2.215002     3      -0.484089   \n",
              "1 -0.211031 -0.791926 -0.267230  0.656657  0.375877     4      -0.109446   \n",
              "2 -0.212212 -0.887437 -0.147594 -0.114876  0.538152     8       0.836733   \n",
              "\n",
              "   TimeOfDay_sin  \n",
              "0      -0.057346  \n",
              "1      -0.652870  \n",
              "2      -0.405208  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert date/time into cyclic features for both train and eval datasets\n",
        "for df in [train_df_norm, eval_df_norm]:\n",
        "    # Extract components from DateFormatted\n",
        "    df['Year'] = df['DateFormatted'].dt.year - 1989\n",
        "    df['DayOfYear'] = df['DateFormatted'].dt.dayofyear\n",
        "    df['TimeOfDay'] = (df['DateFormatted'].dt.hour * 3600 +\n",
        "                       df['DateFormatted'].dt.minute * 60 +\n",
        "                       df['DateFormatted'].dt.second) / 86400\n",
        "\n",
        "    # Calculate cyclic features\n",
        "    df['DayOfYear_sin'] = np.sin(2 * np.pi * df['DayOfYear'] / 365.25)\n",
        "    df['TimeOfDay_sin'] = np.sin(2 * np.pi * df['TimeOfDay'])\n",
        "\n",
        "# Drop original date column and intermediate columns\n",
        "train_df_norm_final = train_df_norm.drop(['DateFormatted', 'DayOfYear', 'TimeOfDay'], axis=1)\n",
        "eval_df_norm_final = eval_df_norm.drop(['DateFormatted', 'DayOfYear', 'TimeOfDay'], axis=1)\n",
        "\n",
        "print(\"Cyclic features added to train and eval datasets.\")\n",
        "train_df_norm_final.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zevfLZ-s4JyH"
      },
      "outputs": [],
      "source": [
        "input_columns = ['Altitude', 'GCLAT', 'GCLON', 'ILAT', 'GLAT', 'GMLT', 'XXLAT', 'XXLON', 'Ne1', 'Pv1', 'I1', 'Year', 'DayOfYear_sin', 'TimeOfDay_sin']\n",
        "output_column = 'Te1'\n",
        "\n",
        "class DataFrameDataset(Dataset):\n",
        "    def __init__(self, dataframe, input_columns, output_column):\n",
        "        self.X = torch.tensor(dataframe[input_columns].values, dtype=torch.float32)\n",
        "        self.y = torch.tensor(dataframe[output_column].values, dtype=torch.float32).reshape(-1, 1)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "class Network(nn.Module): # ff_1\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Network, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "class GaussianNetwork(nn.Module): # gaussian_1\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GaussianNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.mean_layer = nn.Linear(hidden_size, output_size)\n",
        "        self.var_layer = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        mean = self.mean_layer(x)\n",
        "        var = self.relu(self.var_layer(x))\n",
        "        return mean, var\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_epochs = 5\n",
        "max_lr = 4e-4\n",
        "min_lr = 1e-5\n",
        "\n",
        "# Set up data loader\n",
        "train_ds = DataFrameDataset(train_df_norm_final, input_columns, output_column)\n",
        "eval_ds = DataFrameDataset(eval_df_norm_final, input_columns, output_column)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "eval_loader = DataLoader(eval_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# Initialize the model\n",
        "input_size = len(input_columns)\n",
        "hidden_size = 256\n",
        "output_size = 1\n",
        "model = Network(input_size, hidden_size, output_size).to(\"cuda\")\n",
        "# model = GaussianNetwork(input_size, hidden_size, output_size).to(\"cuda\")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "# criterion = nn.GaussianNLLLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=max_lr)\n",
        "\n",
        "# Implement One Cycle LR\n",
        "steps_per_epoch = len(train_loader)\n",
        "total_train_steps = num_epochs * steps_per_epoch\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_lr, \n",
        "                                                total_steps=total_train_steps,\n",
        "                                                pct_start=0.3, anneal_strategy='cos',\n",
        "                                                cycle_momentum=True, base_momentum=0.85,\n",
        "                                                max_momentum=0.95, div_factor=25.0,\n",
        "                                                final_div_factor=10000.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hLwfcGROagC",
        "outputId": "d6643fee-9133-485e-bb83-e54173c2ddf6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/3286 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3286/3286 [00:07<00:00, 411.76it/s]\n",
            "100%|██████████| 782/782 [00:00<00:00, 869.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Avg Train Loss: 0.5218, Test Loss: 0.2188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3286/3286 [00:07<00:00, 415.20it/s]\n",
            "100%|██████████| 782/782 [00:00<00:00, 879.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/5], Avg Train Loss: 0.1521, Test Loss: 0.1093\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3286/3286 [00:07<00:00, 425.39it/s]\n",
            "100%|██████████| 782/782 [00:00<00:00, 797.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/5], Avg Train Loss: 0.0988, Test Loss: 0.0911\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3286/3286 [00:10<00:00, 307.58it/s]\n",
            "100%|██████████| 782/782 [00:00<00:00, 947.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/5], Avg Train Loss: 0.0881, Test Loss: 0.0860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3286/3286 [00:07<00:00, 456.41it/s]\n",
            "100%|██████████| 782/782 [00:00<00:00, 937.05it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/5], Avg Train Loss: 0.0849, Test Loss: 0.0852\n",
            "Training finished!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 782/782 [00:00<00:00, 980.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Test Loss: 0.0852\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize wandb run\n",
        "wandb.init(\n",
        "    project=\"auroral-precipitation-ml\",\n",
        "    config={\n",
        "        \"dataset_size\": len(train_df),\n",
        "        \"validation_size\": len(eval_df),\n",
        "        \"columns\": columns_to_keep\n",
        "    }\n",
        ")\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(data_loader):\n",
        "            x = x.to(\"cuda\")\n",
        "            y = y.to(\"cuda\")\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            # mean, var = model(x)\n",
        "            # loss = criterion(mean, y, var)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# Training loop\n",
        "eval_every = 1\n",
        "\n",
        "total_steps = 0\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    model.train()\n",
        "    for i, (x, y) in enumerate(tqdm(train_loader)):\n",
        "        x = x.to(\"cuda\")\n",
        "        y = y.to(\"cuda\")\n",
        "\n",
        "        # Forward pass\n",
        "        y_pred = model(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "        # mean, var = model(x)\n",
        "        # loss = criterion(mean, y, var)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        total_steps += 1\n",
        "\n",
        "        # Log train loss and learning rate every 100 iterations\n",
        "        if total_steps % 100 == 0:\n",
        "            wandb.log({\n",
        "                \"train_loss\": loss.item(),\n",
        "                \"learning_rate\": scheduler.get_last_lr()[0],\n",
        "                \"total_steps\": total_steps\n",
        "            })\n",
        "\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "    test_loss = evaluate_model(model, eval_loader, criterion)\n",
        "    \n",
        "    # Log train and test loss to wandb\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"test_loss\": test_loss\n",
        "    })\n",
        "    if (epoch + 1) % eval_every == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Avg Train Loss: {avg_train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "print('Training finished!')\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'ff_model.pth')\n",
        "\n",
        "# Final evaluation\n",
        "final_test_loss = evaluate_model(model, eval_loader, criterion)\n",
        "print(f'Final Test Loss: {final_test_loss:.4f}')\n",
        "wandb.log({\"final_test_loss\": final_test_loss})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fovbv-QcPC-2",
        "outputId": "45c77601-257a-4ef4-c4c1-5a525350e0e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_56756/3164102288.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('ff_model.pth'))\n",
            "Evaluating: 100%|██████████| 100000/100000 [00:37<00:00, 2686.86it/s]\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on the test set\n",
        "# # Gaussian Network gaussian_1\n",
        "# model = GaussianNetwork(input_size, hidden_size, output_size).to(\"cuda\")\n",
        "# model.load_state_dict(torch.load('gaussian_model.pth'))\n",
        "\n",
        "# FF Network ff_1\n",
        "model = Network(input_size, hidden_size, output_size).to(\"cuda\")\n",
        "model.load_state_dict(torch.load('ff_model.pth'))\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_df = pd.read_csv('data/test.tsv', sep='\\t')\n",
        "test_df = test_df[columns_to_keep]\n",
        "test_df['DateFormatted'] = pd.to_datetime(test_df['DateFormatted'] + ' ' + test_df['TimeFormatted'], format='%Y-%m-%d %H:%M:%S')\n",
        "test_df.drop('TimeFormatted', axis=1, inplace=True)\n",
        "test_df_norm = normalize_df(test_df, means, stds, columns_to_normalize)\n",
        "\n",
        "# Convert date/time into cyclic features for test dataset\n",
        "# Extract components from DateFormatted\n",
        "test_df_norm['Year'] = test_df_norm['DateFormatted'].dt.year - 1989\n",
        "test_df_norm['DayOfYear'] = test_df_norm['DateFormatted'].dt.dayofyear\n",
        "test_df_norm['TimeOfDay'] = (test_df_norm['DateFormatted'].dt.hour * 3600 +\n",
        "                             test_df_norm['DateFormatted'].dt.minute * 60 +\n",
        "                             test_df_norm['DateFormatted'].dt.second) / 86400\n",
        "\n",
        "# Calculate cyclic features\n",
        "test_df_norm['DayOfYear_sin'] = np.sin(2 * np.pi * test_df_norm['DayOfYear'] / 365.25)\n",
        "test_df_norm['TimeOfDay_sin'] = np.sin(2 * np.pi * test_df_norm['TimeOfDay'])\n",
        "test_df_norm_final = test_df_norm.drop(['DateFormatted', 'DayOfYear', 'TimeOfDay'], axis=1)\n",
        "\n",
        "test_ds = DataFrameDataset(test_df_norm_final, input_columns, output_column)\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "target_mean = means[output_column]\n",
        "target_std = stds[output_column]\n",
        "\n",
        "def unnormalize_mean(pred, target_mean, target_std):\n",
        "    return pred * target_std + target_mean\n",
        "\n",
        "def unnormalize_var(var, target_std):\n",
        "    return var * (target_std ** 2)\n",
        "\n",
        "correct_predictions = 0\n",
        "total_samples = len(test_loader)\n",
        "predictions = []\n",
        "true_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        x = x.to(\"cuda\")\n",
        "        y = y.to(\"cuda\")\n",
        "        \n",
        "        # Forward pass\n",
        "        # y_pred, var = model(x)\n",
        "        y_pred = model(x)\n",
        "        \n",
        "        # Unnormalize predictions and true values\n",
        "        y_true = unnormalize_mean(y.cpu().item(), target_mean, target_std)\n",
        "        y_pred = unnormalize_mean(y_pred.cpu().item(), target_mean, target_std)\n",
        "        # var_pred = unnormalize_var(var.cpu().item(), target_std) # Not used but could be useful for uncertainty quantification\n",
        "        \n",
        "        predictions.append(y_pred)\n",
        "        true_values.append(y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate deviations\n",
        "deviations = [pred - true for pred, true in zip(predictions, true_values)]\n",
        "\n",
        "# Calculate percentages within specified absolute deviations\n",
        "thresholds = [100, 200, 300, 500, 1000, 2000, 5000]\n",
        "percentages = [\n",
        "    sum(abs(dev) <= threshold for dev in deviations) / len(deviations) * 100\n",
        "    for threshold in thresholds\n",
        "]\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.hist(deviations, bins=50, edgecolor='black')\n",
        "plt.xlabel('Deviation from Ground Truth')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Model Predictions Deviation')\n",
        "\n",
        "# Add text box with percentages\n",
        "text = \"\\n\".join([\n",
        "    f\"Within {threshold}: {percentage:.2f}%\"\n",
        "    for threshold, percentage in zip(thresholds, percentages)\n",
        "])\n",
        "plt.text(0.95, 0.95, text, transform=plt.gca().transAxes, \n",
        "         verticalalignment='top', horizontalalignment='right',\n",
        "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig('deviation.png')\n",
        "plt.close()  # Close the figure to free up memory"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
